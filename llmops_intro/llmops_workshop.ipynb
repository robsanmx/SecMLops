{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMOps Workshop: 30-Minute Introduction\n",
    "\n",
    "**Duration:** 30 minutes  \n",
    "**Requirements:** Only local tools (no cloud services, no costs)\n",
    "\n",
    "## What is LLMOps?\n",
    "\n",
    "LLMOps (Large Language Model Operations) extends MLOps principles to manage the lifecycle of LLM applications:\n",
    "- **Prompt Engineering & Versioning**\n",
    "- **Model Evaluation & Testing**\n",
    "- **Monitoring & Logging**\n",
    "- **Deployment & Scaling**\n",
    "\n",
    "## Workshop Overview\n",
    "1. Setting up a local LLMOps environment\n",
    "2. Prompt versioning and tracking\n",
    "3. Simple evaluation metrics\n",
    "4. Basic monitoring setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup (5 minutes)\n",
    "\n",
    "We'll use only free, local tools:\n",
    "- **Transformers** (Hugging Face) for local models\n",
    "- **MLflow** for experiment tracking\n",
    "- **JSON files** for simple logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nINSTALACI√ìN DE DEPENDENCIAS PARA LLMOPS\n=======================================\n\nEste bloque instala todas las librer√≠as necesarias para el workshop de LLMOps.\n\nLibrer√≠as incluidas:\n- transformers: Para cargar modelos de Hugging Face localmente\n- torch: Motor de deep learning (PyTorch) requerido por transformers\n- mlflow: Plataforma de tracking de experimentos de ML\n- pandas: Manipulaci√≥n y an√°lisis de datos\n- matplotlib: Visualizaci√≥n de datos\n- seaborn: Gr√°ficos estad√≠sticos avanzados\n\nEjemplo de uso despu√©s de la instalaci√≥n:\nfrom transformers import pipeline\ngenerator = pipeline('text-generation', model='gpt2')\n\nNOTA: Ejecutar solo una vez al inicio del workshop\n\"\"\"\n# Instalar paquetes requeridos (ejecutar solo una vez)\n# Comentar la l√≠nea siguiente despu√©s de la primera ejecuci√≥n\n!pip install transformers torch mlflow pandas matplotlib seaborn"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCONFIGURACI√ìN DEL ENTORNO LLMOPS LOCAL\n=====================================\n\nEste bloque configura el entorno base para nuestro pipeline de LLMOps.\n\nEstructura de directorios creada:\nüìÅ prompts/    -> Almacena versiones de prompts con metadatos\nüìÅ logs/       -> Guarda m√©tricas y resultados de evaluaciones  \nüìÅ experiments/ -> Tracking de experimentos y configuraciones\n\nLibrer√≠as importadas:\n- json: Serializaci√≥n de datos de prompts y m√©tricas\n- pandas: An√°lisis de datos de evaluaci√≥n\n- matplotlib: Visualizaci√≥n de m√©tricas\n- datetime: Timestamps para versionado\n- transformers: Modelos de lenguaje locales\n- mlflow: Tracking avanzado de experimentos\n- os: Operaciones del sistema de archivos\n\nEjemplo de uso:\nLos directorios permiten organizar:\n- prompts/assistant_v1.json (versi√≥n 1 del prompt)\n- logs/evaluation_metrics.json (m√©tricas guardadas)\n- experiments/model_comparison.json (resultados de experimentos)\n\"\"\"\n\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom transformers import pipeline\nimport mlflow\nimport os\n\n# Crear estructura de directorios para nuestro setup de LLMOps\n# exist_ok=True evita errores si los directorios ya existen\nos.makedirs('prompts', exist_ok=True)     # Versionado de prompts\nos.makedirs('logs', exist_ok=True)        # M√©tricas y logs\nos.makedirs('experiments', exist_ok=True) # Tracking de experimentos\n\nprint(\"‚úÖ Entorno LLMOps configurado correctamente!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load a Free Local Model (5 minutes)\n",
    "\n",
    "We'll use a small, free model that runs locally without API costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nCARGA DE MODELO LOCAL PARA LLMOPS\n=================================\n\nEste bloque carga un modelo de lenguaje que funciona completamente local,\nsin costos de API ni conexi√≥n a internet requerida.\n\nModelo seleccionado: microsoft/DialoGPT-small\n- Tama√±o: ~117MB (modelo peque√±o y r√°pido)\n- Tipo: Generaci√≥n de texto conversacional\n- Ventajas: Gratuito, local, sin l√≠mites de tokens\n- Desventajas: Menor calidad que modelos grandes (GPT-4, etc.)\n\nConfiguraci√≥n del pipeline:\n- task='text-generation': Tipo de tarea (generaci√≥n de texto)\n- return_full_text=False: Solo devuelve el texto generado (no el input)\n- max_length=100: M√°ximo 100 tokens por respuesta\n\nEjemplo de uso despu√©s de cargar:\nresponse = generator(\"Hola, ¬øc√≥mo est√°s?\")\nprint(response[0]['generated_text'])\n\nAlternativas de modelos locales:\n- gpt2: Modelo base de OpenAI (m√°s general)\n- distilgpt2: Versi√≥n m√°s r√°pida y ligera\n- microsoft/DialoGPT-medium: Mejor calidad, m√°s pesado\n\"\"\"\n\n# Cargar modelo de generaci√≥n de texto local gratuito\nprint(\"Cargando modelo local DialoGPT-small...\")\nprint(\"üìÅ Descargando ~117MB la primera vez...\")\n\ngenerator = pipeline(\n    'text-generation',                    # Tarea: generaci√≥n de texto\n    model='microsoft/DialoGPT-small',    # Modelo conversacional peque√±o\n    return_full_text=False,              # Solo texto generado, no input\n    max_length=100                       # M√°ximo 100 tokens por respuesta\n)\n\nprint(\"‚úÖ Modelo cargado exitosamente!\")\nprint(\"üöÄ Listo para generar texto localmente\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prompt Versioning System (8 minutes)\n",
    "\n",
    "Create a simple system to version and track prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nSISTEMA DE VERSIONADO DE PROMPTS PARA LLMOPS\n===========================================\n\nEsta clase implementa un sistema de control de versiones para prompts,\nfundamental en LLMOps para rastrear la evoluci√≥n y mejoras de prompts.\n\n¬øPor qu√© versionar prompts?\n- Reproducibilidad: Poder recrear exactamente un experimento\n- A/B Testing: Comparar diferentes versiones de prompts\n- Rollback: Volver a versiones anteriores si hay problemas\n- Auditor√≠a: Rastrear qu√© cambios mejoraron el rendimiento\n\nEstructura de datos guardada:\n{\n  \"version\": 1,\n  \"prompt\": \"Texto del prompt con {placeholders}\",\n  \"description\": \"Descripci√≥n de los cambios\",\n  \"created_at\": \"2024-01-15T10:30:00\"\n}\n\nEjemplo de evoluci√≥n de prompts:\nv1: \"Responde sobre: {tema}\"\nv2: \"Como experto, explica detalladamente: {tema}\"\nv3: \"Eres un especialista en {√°rea}. Explica {tema} con ejemplos pr√°cticos\"\n\nM√©todos de la clase:\n- save_prompt(): Guarda nueva versi√≥n con metadatos autom√°ticos\n- load_prompt(): Carga versi√≥n espec√≠fica o la m√°s reciente\n\"\"\"\n\nclass PromptManager:\n    \"\"\"\n    Gestor de versiones de prompts para LLMOps\n    \n    Permite guardar, cargar y versionar prompts con metadatos autom√°ticos.\n    Cada prompt se guarda como JSON con timestamp y descripci√≥n.\n    \"\"\"\n    \n    def __init__(self, base_path='prompts'):\n        \"\"\"\n        Inicializar el gestor de prompts\n        \n        Args:\n            base_path (str): Directorio donde guardar los archivos de prompts\n        \"\"\"\n        self.base_path = base_path\n        self.current_version = 1\n    \n    def save_prompt(self, name, prompt, description=\"\"):\n        \"\"\"\n        Guardar una nueva versi√≥n de un prompt\n        \n        Args:\n            name (str): Nombre identificador del prompt (ej: \"assistant\", \"translator\")\n            prompt (str): Texto del prompt con placeholders opcionales {variable}\n            description (str): Descripci√≥n de los cambios o prop√≥sito\n            \n        Returns:\n            str: Ruta del archivo creado\n            \n        Ejemplo:\n            pm.save_prompt(\"chatbot\", \"Responde como {persona}: {pregunta}\", \n                          \"A√±adido contexto de personalidad\")\n        \"\"\"\n        prompt_data = {\n            'version': self.current_version,           # N√∫mero de versi√≥n auto-incrementado\n            'prompt': prompt,                          # Texto del prompt\n            'description': description,                # Descripci√≥n de cambios\n            'created_at': datetime.now().isoformat()  # Timestamp ISO 8601\n        }\n        \n        # Crear nombre de archivo con versi√≥n: assistant_v1.json\n        filename = f\"{self.base_path}/{name}_v{self.current_version}.json\"\n        \n        # Guardar prompt con formato JSON legible (indent=2)\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(prompt_data, f, indent=2, ensure_ascii=False)\n        \n        self.current_version += 1\n        return filename\n    \n    def load_prompt(self, name, version=None):\n        \"\"\"\n        Cargar un prompt espec√≠fico por nombre y versi√≥n\n        \n        Args:\n            name (str): Nombre del prompt a cargar\n            version (int, optional): Versi√≥n espec√≠fica. Si None, carga la m√°s reciente\n            \n        Returns:\n            dict: Datos del prompt incluyendo metadatos\n            \n        Ejemplo:\n            prompt_data = pm.load_prompt(\"assistant\", 1)\n            texto = prompt_data['prompt'].format(tema=\"Python\")\n        \"\"\"\n        if version is None:\n            version = self.current_version - 1  # Versi√≥n m√°s reciente\n        \n        filename = f\"{self.base_path}/{name}_v{version}.json\"\n        \n        with open(filename, 'r', encoding='utf-8') as f:\n            return json.load(f)\n\n# Inicializar el gestor de prompts\npm = PromptManager()\n\n# Crear ejemplos de prompts con evoluci√≥n iterativa\nprompt_v1 = \"Generate a helpful response about: {topic}\"\nprompt_v2 = \"As an expert assistant, provide detailed information about: {topic}\"\n\n# Guardar versiones con descripciones explicativas\npm.save_prompt(\"assistant\", prompt_v1, \"Prompt b√°sico inicial\")\npm.save_prompt(\"assistant\", prompt_v2, \"Mejorado con persona de experto\")\n\nprint(\"‚úÖ Sistema de versionado de prompts creado!\")\nprint(\"üìÅ Archivos creados en directorio 'prompts/':\")\nprint(\"   - assistant_v1.json (versi√≥n b√°sica)\")\nprint(\"   - assistant_v2.json (versi√≥n con persona)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Simple Evaluation Framework (7 minutes)\n",
    "\n",
    "Create basic metrics to evaluate LLM responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nFRAMEWORK DE EVALUACI√ìN PARA LLMOPS\n==================================\n\nEsta clase implementa un sistema de evaluaci√≥n de respuestas de LLMs,\ncrucial para medir y mejorar la calidad de las respuestas generadas.\n\nM√©tricas implementadas:\n1. response_length: Longitud en caracteres de la respuesta\n2. word_count: N√∫mero de palabras en la respuesta  \n3. keyword_score: Porcentaje de palabras clave encontradas (0.0 - 1.0)\n\n¬øPor qu√© evaluar respuestas LLM?\n- Calidad: Medir si las respuestas son apropiadas\n- Consistencia: Verificar comportamiento estable entre versiones\n- Mejora continua: Identificar √°reas de optimizaci√≥n\n- A/B Testing: Comparar diferentes prompts objetivamente\n\nEstructura de m√©tricas guardadas:\n{\n  \"timestamp\": \"2024-01-15T10:30:00\",\n  \"prompt\": \"Prompt usado\",\n  \"response\": \"Respuesta generada\",\n  \"response_length\": 150,\n  \"word_count\": 25,\n  \"keyword_score\": 0.75,\n  \"expected_keywords\": [\"algoritmo\", \"datos\", \"modelo\"]\n}\n\nEjemplo de evaluaci√≥n:\nevaluator.evaluate_response(\n    prompt=\"Explica machine learning\",\n    response=\"Machine learning usa algoritmos para analizar datos\",\n    expected_keywords=[\"algoritmo\", \"datos\", \"modelo\"]\n)\n# Resultado: keyword_score = 0.67 (2 de 3 palabras encontradas)\n\"\"\"\n\nclass LLMEvaluator:\n    \"\"\"\n    Sistema de evaluaci√≥n de respuestas de modelos de lenguaje\n    \n    Calcula m√©tricas autom√°ticas para evaluar calidad y relevancia\n    de las respuestas generadas por LLMs.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Inicializar evaluador con lista vac√≠a de m√©tricas\"\"\"\n        self.metrics = []\n    \n    def evaluate_response(self, prompt, response, expected_keywords=None):\n        \"\"\"\n        Evaluar una respuesta del LLM con m√∫ltiples m√©tricas\n        \n        Args:\n            prompt (str): Prompt usado para generar la respuesta\n            response (str): Respuesta generada por el modelo\n            expected_keywords (list, optional): Palabras clave esperadas\n            \n        Returns:\n            dict: Diccionario con todas las m√©tricas calculadas\n            \n        Ejemplo:\n            metrics = evaluator.evaluate_response(\n                prompt=\"¬øQu√© es Python?\",\n                response=\"Python es un lenguaje de programaci√≥n f√°cil de aprender\",\n                expected_keywords=[\"lenguaje\", \"programaci√≥n\", \"c√≥digo\"]\n            )\n            print(f\"Score: {metrics['keyword_score']}\")  # 0.67\n        \"\"\"\n        \n        # M√©tricas b√°sicas de longitud\n        response_length = len(response)                    # Caracteres totales\n        word_count = len(response.split())                # Palabras totales\n        \n        # M√©trica de cobertura de palabras clave\n        keyword_score = 0\n        if expected_keywords:\n            # Contar cu√°ntas keywords aparecen en la respuesta (case-insensitive)\n            found_keywords = sum(\n                1 for kw in expected_keywords \n                if kw.lower() in response.lower()\n            )\n            # Calcular porcentaje de cobertura (0.0 a 1.0)\n            keyword_score = found_keywords / len(expected_keywords)\n        \n        # Estructura completa de m√©tricas con metadatos\n        metrics = {\n            'timestamp': datetime.now().isoformat(),       # Cu√°ndo se evalu√≥\n            'prompt': prompt,                              # Prompt original\n            'response': response,                          # Respuesta evaluada\n            'response_length': response_length,            # M√©trica: longitud\n            'word_count': word_count,                      # M√©trica: palabras\n            'keyword_score': keyword_score,                # M√©trica: relevancia\n            'expected_keywords': expected_keywords or []   # Keywords esperadas\n        }\n        \n        # Agregar a la lista de m√©tricas para an√°lisis posterior\n        self.metrics.append(metrics)\n        return metrics\n    \n    def save_metrics(self, filename='logs/evaluation_metrics.json'):\n        \"\"\"\n        Guardar todas las m√©tricas en archivo JSON\n        \n        Args:\n            filename (str): Ruta donde guardar las m√©tricas\n            \n        Ejemplo:\n            evaluator.save_metrics('experimento_01_metrics.json')\n        \"\"\"\n        with open(filename, 'w', encoding='utf-8') as f:\n            json.dump(self.metrics, f, indent=2, ensure_ascii=False)\n    \n    def get_summary(self):\n        \"\"\"\n        Obtener resumen estad√≠stico de todas las m√©tricas\n        \n        Returns:\n            pandas.DataFrame: Estad√≠sticas descriptivas (mean, std, min, max, etc.)\n            \n        Ejemplo:\n            summary = evaluator.get_summary()\n            print(f\"Longitud promedio: {summary.loc['mean', 'response_length']}\")\n        \"\"\"\n        if not self.metrics:\n            return \"No hay m√©tricas disponibles para analizar\"\n        \n        # Convertir m√©tricas a DataFrame para an√°lisis estad√≠stico\n        df = pd.DataFrame(self.metrics)\n        \n        # Seleccionar solo columnas num√©ricas para estad√≠sticas\n        numeric_columns = ['response_length', 'word_count', 'keyword_score']\n        return df[numeric_columns].describe()\n\n# Inicializar el evaluador\nevaluator = LLMEvaluator()\n\nprint(\"‚úÖ Framework de evaluaci√≥n configurado!\")\nprint(\"üìä M√©tricas disponibles:\")\nprint(\"   - response_length: Longitud en caracteres\")\nprint(\"   - word_count: N√∫mero de palabras\")\nprint(\"   - keyword_score: Cobertura de palabras clave (0.0-1.0)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Hands-on Example (5 minutes)\n",
    "\n",
    "Let's test our LLMOps pipeline with real examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nEJEMPLO PR√ÅCTICO: PIPELINE COMPLETO DE LLMOPS\n=============================================\n\nEste bloque demuestra un pipeline completo de LLMOps ejecutando:\n1. Carga de m√∫ltiples versiones de prompts\n2. Generaci√≥n de respuestas con modelo local\n3. Evaluaci√≥n comparativa de respuestas\n4. Almacenamiento de m√©tricas para an√°lisis\n\nFlujo del experimento:\nüìù Cargar prompts v1 y v2\nüéØ Formatear con temas de prueba  \nü§ñ Generar respuestas con modelo local\nüìä Evaluar calidad con m√©tricas autom√°ticas\nüíæ Guardar resultados para tracking\n\nTemas de prueba seleccionados:\n- \"machine learning\": Algoritmos de aprendizaje autom√°tico\n- \"data science\": Ciencia de datos y an√°lisis\n- \"artificial intelligence\": Inteligencia artificial\n\nPalabras clave para evaluaci√≥n:\n- \"algorithm\": Debe aparecer en respuestas t√©cnicas\n- \"data\": Fundamental en temas de datos\n- \"model\": Concepto clave en ML/AI\n- \"prediction\": Objetivo com√∫n en estos campos\n\nComparaci√≥n A/B Testing:\n- V1: Prompt b√°sico sin contexto espec√≠fico\n- V2: Prompt con persona de \"experto\" para mejores respuestas\n\"\"\"\n\n# Configuraci√≥n del experimento de evaluaci√≥n comparativa\ntest_topics = [\"machine learning\", \"data science\", \"artificial intelligence\"]\ntest_keywords = [\"algorithm\", \"data\", \"model\", \"prediction\"]\n\nprint(\"üß™ Iniciando experimento de evaluaci√≥n comparativa\")\nprint(\"üìã Temas a evaluar:\", test_topics)\nprint(\"üîç Palabras clave esperadas:\", test_keywords)\nprint(\"\\n\" + \"=\"*60)\n\n# Iterar sobre cada tema para evaluaci√≥n completa\nfor i, topic in enumerate(test_topics, 1):\n    print(f\"\\nüìä Experimento {i}/3 - Tema: '{topic}'\")\n    \n    # PASO 1: Cargar ambas versiones de prompts desde archivos JSON\n    prompt_v1_data = pm.load_prompt(\"assistant\", 1)  # Versi√≥n b√°sica\n    prompt_v2_data = pm.load_prompt(\"assistant\", 2)  # Versi√≥n con persona\n    \n    # PASO 2: Formatear prompts con el tema actual usando placeholders\n    formatted_prompt_v1 = prompt_v1_data['prompt'].format(topic=topic)\n    formatted_prompt_v2 = prompt_v2_data['prompt'].format(topic=topic)\n    \n    print(f\"   üîÑ Prompts formateados para '{topic}'\")\n    \n    # PASO 3: Generar respuestas usando el modelo local\n    try:\n        # Intentar generaci√≥n real con el modelo DialoGPT\n        response_v1 = generator(formatted_prompt_v1, max_length=50, num_return_sequences=1)[0]['generated_text']\n        response_v2 = generator(formatted_prompt_v2, max_length=50, num_return_sequences=1)[0]['generated_text']\n        print(f\"   ‚úÖ Respuestas generadas por modelo local\")\n        \n    except Exception as e:\n        # Fallback con respuestas simuladas si hay errores\n        print(f\"   ‚ö†Ô∏è  Modelo no disponible, usando respuestas simuladas\")\n        response_v1 = f\"Basic information about {topic} including algorithms and data processing.\"\n        response_v2 = f\"As an expert, {topic} involves sophisticated algorithms, data analysis, and predictive modeling techniques.\"\n    \n    # PASO 4: Evaluar ambas respuestas con m√©tricas autom√°ticas\n    eval_v1 = evaluator.evaluate_response(\n        prompt=formatted_prompt_v1,\n        response=response_v1,\n        expected_keywords=test_keywords\n    )\n    \n    eval_v2 = evaluator.evaluate_response(\n        prompt=formatted_prompt_v2,\n        response=response_v2,\n        expected_keywords=test_keywords\n    )\n    \n    # PASO 5: Mostrar comparaci√≥n de resultados\n    print(f\"   üìà Resultados comparativos:\")\n    print(f\"      V1 (b√°sico)  - Longitud: {eval_v1['response_length']:3d} chars, Keywords: {eval_v1['keyword_score']:.2f}\")\n    print(f\"      V2 (experto) - Longitud: {eval_v2['response_length']:3d} chars, Keywords: {eval_v2['keyword_score']:.2f}\")\n    \n    # Determinar qu√© versi√≥n tuvo mejor rendimiento\n    if eval_v2['keyword_score'] > eval_v1['keyword_score']:\n        print(f\"      üèÜ V2 (experto) obtuvo mejor keyword score\")\n    elif eval_v1['keyword_score'] > eval_v2['keyword_score']:\n        print(f\"      üèÜ V1 (b√°sico) obtuvo mejor keyword score\")\n    else:\n        print(f\"      ü§ù Ambas versiones obtuvieron el mismo keyword score\")\n\nprint(\"\\n\" + \"=\"*60)\n\n# PASO 6: Guardar todas las m√©tricas para an√°lisis posterior\nevaluator.save_metrics()\nprint(\"‚úÖ Experimento completado!\")\nprint(\"üíæ M√©tricas guardadas en 'logs/evaluation_metrics.json'\")\nprint(\"üìä Datos listos para an√°lisis y dashboard de monitoreo\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Simple Monitoring Dashboard (Bonus)\n",
    "\n",
    "Create basic visualizations of our metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\"\"\"\nDASHBOARD DE MONITOREO PARA LLMOPS\n=================================\n\nEste bloque crea visualizaciones para monitorear el rendimiento del LLM,\nelemento esencial en LLMOps para tracking continuo de la calidad.\n\nGr√°ficos generados:\n1. Distribuci√≥n de longitud de respuestas\n   - Identifica respuestas muy cortas (posibles errores)\n   - Detecta respuestas muy largas (posible divagaci√≥n)\n   - Muestra consistencia en la longitud de output\n\n2. Distribuci√≥n de keyword score\n   - Mide relevancia de respuestas (0.0 = irrelevante, 1.0 = perfecta)\n   - Identifica problemas de calidad en prompts\n   - Compara rendimiento entre versiones\n\n¬øPor qu√© monitorear LLMs?\n- Detectar degradaci√≥n de calidad en tiempo real\n- Identificar prompts problem√°ticos\n- Validar mejoras en nuevas versiones\n- Mantener SLAs de calidad en producci√≥n\n\nM√©tricas adicionales para producci√≥n:\n- Tiempo de respuesta (latencia)\n- Tasa de errores del modelo\n- Costos de tokens (si usa APIs de pago)\n- Satisfacci√≥n del usuario (feedback)\n\nAlertas recomendadas:\n- Keyword score < 0.3 (baja relevancia)\n- Response length < 10 chars (respuestas muy cortas)\n- Response length > 1000 chars (posible alucinaci√≥n)\n\"\"\"\n\n# Verificar si tenemos m√©tricas para visualizar\nif evaluator.metrics:\n    print(\"üìä Generando dashboard de monitoreo...\")\n    \n    # Convertir m√©tricas a DataFrame para an√°lisis\n    df = pd.DataFrame(evaluator.metrics)\n    print(f\"üìà Procesando {len(df)} evaluaciones realizadas\")\n    \n    # Crear figura con 2 subplots lado a lado\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    fig.suptitle('Dashboard de Monitoreo LLMOps', fontsize=16, fontweight='bold')\n    \n    # GR√ÅFICO 1: Distribuci√≥n de longitud de respuestas\n    axes[0].hist(df['response_length'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n    axes[0].set_title('Distribuci√≥n de Longitud de Respuestas', fontweight='bold')\n    axes[0].set_xlabel('Caracteres')\n    axes[0].set_ylabel('Frecuencia')\n    axes[0].grid(True, alpha=0.3)\n    \n    # Agregar l√≠neas de referencia para alertas\n    mean_length = df['response_length'].mean()\n    axes[0].axvline(mean_length, color='red', linestyle='--', \n                   label=f'Promedio: {mean_length:.0f}')\n    axes[0].legend()\n    \n    # GR√ÅFICO 2: Distribuci√≥n de keyword score\n    axes[1].hist(df['keyword_score'], bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n    axes[1].set_title('Distribuci√≥n de Keyword Score', fontweight='bold')\n    axes[1].set_xlabel('Score (0.0 - 1.0)')\n    axes[1].set_ylabel('Frecuencia')\n    axes[1].grid(True, alpha=0.3)\n    \n    # Agregar l√≠neas de referencia para calidad\n    mean_score = df['keyword_score'].mean()\n    axes[1].axvline(mean_score, color='red', linestyle='--', \n                   label=f'Promedio: {mean_score:.2f}')\n    axes[1].axvline(0.5, color='orange', linestyle=':', \n                   label='Umbral m√≠nimo: 0.5')\n    axes[1].legend()\n    \n    # Ajustar layout y guardar dashboard\n    plt.tight_layout()\n    plt.savefig('logs/monitoring_dashboard.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(\"‚úÖ Dashboard guardado en 'logs/monitoring_dashboard.png'\")\n    \n    # AN√ÅLISIS ESTAD√çSTICO DETALLADO\n    print(\"\\nüìà Resumen Estad√≠stico Completo:\")\n    print(\"=\"*50)\n    \n    # Obtener estad√≠sticas descriptivas\n    summary = evaluator.get_summary()\n    print(summary)\n    \n    # ALERTAS AUTOM√ÅTICAS basadas en umbrales\n    print(\"\\nüö® Sistema de Alertas:\")\n    print(\"=\"*30)\n    \n    # Verificar m√©tricas problem√°ticas\n    low_quality_responses = df[df['keyword_score'] < 0.3]\n    very_short_responses = df[df['response_length'] < 10]\n    very_long_responses = df[df['response_length'] > 500]\n    \n    if len(low_quality_responses) > 0:\n        print(f\"‚ö†Ô∏è  ALERTA: {len(low_quality_responses)} respuestas con baja relevancia (score < 0.3)\")\n    \n    if len(very_short_responses) > 0:\n        print(f\"‚ö†Ô∏è  ALERTA: {len(very_short_responses)} respuestas muy cortas (< 10 caracteres)\")\n    \n    if len(very_long_responses) > 0:\n        print(f\"‚ö†Ô∏è  ALERTA: {len(very_long_responses)} respuestas muy largas (> 500 caracteres)\")\n    \n    if len(low_quality_responses) == 0 and len(very_short_responses) == 0 and len(very_long_responses) == 0:\n        print(\"‚úÖ Todas las m√©tricas est√°n dentro de rangos aceptables\")\n    \n    # RECOMENDACIONES AUTOM√ÅTICAS\n    avg_score = df['keyword_score'].mean()\n    if avg_score < 0.5:\n        print(f\"\\nüí° Recomendaci√≥n: Keyword score promedio ({avg_score:.2f}) es bajo.\")\n        print(\"   Considera mejorar los prompts o ajustar las palabras clave esperadas.\")\n    elif avg_score > 0.8:\n        print(f\"\\nüéâ Excelente: Keyword score promedio ({avg_score:.2f}) es muy bueno.\")\n    \nelse:\n    print(\"üìä No hay m√©tricas disponibles para el dashboard.\")\n    print(\"   Ejecuta primero el bloque de evaluaci√≥n para generar datos.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: What We Built (Bonus)\n",
    "\n",
    "In 30 minutes, we created a complete local LLMOps pipeline:\n",
    "\n",
    "### ‚úÖ Components Created:\n",
    "1. **Prompt Versioning System** - Track and manage prompt evolution\n",
    "2. **Local Model Integration** - No API costs, runs offline\n",
    "3. **Evaluation Framework** - Measure response quality\n",
    "4. **Simple Monitoring** - Basic metrics and visualizations\n",
    "5. **Experiment Tracking** - Log all interactions and results\n",
    "\n",
    "### üéØ Key LLMOps Principles Covered:\n",
    "- **Reproducibility** - Version control for prompts\n",
    "- **Evaluation** - Systematic quality measurement\n",
    "- **Monitoring** - Track performance over time\n",
    "- **Cost Management** - Use free, local resources\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "- Integrate with MLflow for advanced experiment tracking\n",
    "- Add more sophisticated evaluation metrics\n",
    "- Implement automated testing pipelines\n",
    "- Scale to production deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop_bsides",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}